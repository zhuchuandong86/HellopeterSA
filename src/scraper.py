import asyncio
import json
import random
import pandas as pd
from datetime import datetime, timedelta
from playwright.async_api import async_playwright
from src.config import Config

async def run_scraper():
    print(f"ğŸ•·ï¸ [Step 1] å¯åŠ¨çˆ¬è™« | ç›®æ ‡ï¼š{Config.TARGET_OPERATORS} | èŒƒå›´ï¼šæœ€è¿‘ {Config.DAYS_TO_SCRAPE} å¤©")
    cutoff_date = datetime.now() - timedelta(days=Config.DAYS_TO_SCRAPE)
    all_data = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        for company in Config.TARGET_OPERATORS:
            print(f"\nğŸ¢ æ­£åœ¨å¤„ç†: {company}")
            page_num = 1
            stop_company = False

            while not stop_company:
                url = f"https://api.hellopeter.com/consumer/business/{company}/reviews?page={page_num}"
                try:
                    await page.goto(url, timeout=30000, wait_until="domcontentloaded")
                    # è·å–é¡µé¢æ–‡æœ¬å†…å®¹è€Œä¸æ˜¯HTMLï¼Œå› ä¸ºAPIè¿”å›JSON
                    content = await page.evaluate("() => document.body.innerText")

                    try:
                        reviews = json.loads(content).get('data', [])
                    except:
                        reviews = []

                    if not reviews:
                        print("   -> æ— æ›´å¤šæ•°æ®ï¼Œåœæ­¢è¯¥è¿è¥å•†ã€‚")
                        break

                    valid_count = 0
                    for item in reviews:
                        created_at = item.get('created_at', '')
                        try:
                            review_date = datetime.strptime(created_at, "%Y-%m-%d %H:%M:%S")
                        except:
                            review_date = datetime.now()

                        if review_date < cutoff_date:
                            stop_company = True
                            continue 

                        all_data.append({
                            "Operator": company,
                            "Date": review_date,
                            "Title": item.get('review_title', ''),
                            "Content": item.get('review_content', ''),
                            "Raw_Rating": item.get('review_rating', 0),
                            # å°è¯•æ„å»ºURLï¼Œé€»è¾‘å–è‡ªåŸä»£ç 
                            "Url": f"https://www.hellopeter.com/{company}/reviews/review-{item.get('id')}"
                        })
                        valid_count += 1

                    if valid_count > 0:
                        print(f"   ç¬¬ {page_num} é¡µ: æŠ“å– {valid_count} æ¡")
                        page_num += 1
                        await asyncio.sleep(random.uniform(0.5, 1.5))
                    else:
                        stop_company = True

                except Exception as e:
                    print(f"   âŒ é”™è¯¯: {e}")
                    break

        await browser.close()

    if all_data:
        df = pd.DataFrame(all_data)
        # ä¿å­˜ä¸­é—´æ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•ï¼Œä¹Ÿç¬¦åˆä½ åŸæœ‰çš„æµç¨‹
        df.to_csv(Config.RAW_FILE, index=False, encoding='utf-8-sig')
        print(f"\nâœ… [Step 1 å®Œæˆ] æ•°æ®å·²ä¿å­˜è‡³ {Config.RAW_FILE} (å…± {len(df)} æ¡)")
        return df
    else:
        print("\nâš ï¸ [Step 1 è­¦å‘Š] æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ã€‚")
        return pd.DataFrame()
